{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = pd.read_csv('train_set.csv')\n",
    "print(f\"Length train set: {len(train_ds)}\")\n",
    "test_ds = pd.read_csv('test_set.csv')\n",
    "print(f\"Length test set: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.groupby(['Label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.groupby(['Label']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found some Deutsch job offers. Removed since they are outliers\n",
    "# stop_words_de = sw.words('german')\n",
    "#Hard removing them\n",
    "remove_jobs = [12, 32, 569, 834, 893, 1256, 1261]\n",
    "train_ds = train_ds.drop(remove_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download if needed\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stop_words = sw.words('italian')\n",
    "stop_words_eng = sw.words('english')\n",
    "\n",
    "\n",
    "commonverbs = list(pd.read_csv('commonverbs.csv',header=None).values[0])\n",
    "frequent = list(pd.read_csv('frequentwords.csv',header=None).values[0])\n",
    "# frequent = []\n",
    "# commonverbs = []\n",
    "\n",
    "include_words = ['bene','bel','buon','r&d','ar','vr']\n",
    "for i in include_words:\n",
    "    if i in stop_words:\n",
    "        stop_words.remove(i)\n",
    "\n",
    "\n",
    "stop_words= stop_words+stop_words_eng+frequent+commonverbs\n",
    "stop_words = set(stop_words)\n",
    "include_words = set(include_words)\n",
    "\n",
    "#it_core_news_sm, md, lg\n",
    "lemmatizer = spacy.load(\"it_core_news_lg\", disable=[\"tagger\", \"parser\", \"ner\"])#\"tokenizer\",\"tagger\", \"parser\", \"ner\", \"textcat\"\n",
    "def CleanSentence(sentence,sw=stop_words):\n",
    "    sentence = sentence.lower()\n",
    "    #Remove all non-alphanumeric character, excluding '&' (like: R&D)\n",
    "    sentence = re.sub(\"[^\\w&-]+|_|\\d+\", \" \", sentence)\n",
    "    sentence = re.sub(\"-\", \"\", sentence)\n",
    "    \n",
    "    lemmas = lemmatizer(sentence)\n",
    "    newSentence = \"\"\n",
    "    removed_c = []\n",
    "    min = 3\n",
    "    max = 16\n",
    "    for lemma in lemmas:\n",
    "        word = lemma.lemma_\n",
    "        if word not in stop_words:\n",
    "            if  (min <= len(word) <= max or word in include_words):\n",
    "                newSentence = newSentence + word + \" \"\n",
    "            else:\n",
    "                removed_c.append(word)\n",
    "    if(False):\n",
    "        if(len(removed_c)>0):\n",
    "            print(removed_c)\n",
    "            print(\"--- ---- ----\")\n",
    "    return newSentence\n",
    "\n",
    "def CleanText(text):\n",
    "    sentences = []\n",
    "    for row in text:\n",
    "        sentences.append(CleanSentence(row))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['clean'] = CleanText(train_ds['Job_offer'])\n",
    "test_ds['clean'] = CleanText(test_ds['Job_offer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = []\n",
    "# for i,s in enumerate(train_ds['clean']):\n",
    "#     l = len(s.split(' '))\n",
    "#     if(l<=3):\n",
    "#         ind.append(i)\n",
    "#         print(i,l,s,train_ds['Label'][i])\n",
    "# train_ds = train_ds.drop(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test preprocessing step\n",
    "ind = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds['Job_offer'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds['clean'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many different words there are\n",
    "counter_text = []\n",
    "words_text_tr = {}\n",
    "ds = train_ds['clean']\n",
    "for s in ds:\n",
    "    counter_text.append(len(s.split()))\n",
    "    for w in s.split():\n",
    "        if w not in words_text_tr:\n",
    "            words_text_tr[w] = 1\n",
    "        words_text_tr[w] = words_text_tr[w] + 1\n",
    "print(f'Different words in training: {len(words_text_tr)}')\n",
    "\n",
    "counter_text = []\n",
    "words_text_te = {}\n",
    "ds = test_ds['clean']\n",
    "for s in ds:\n",
    "    counter_text.append(len(s.split()))\n",
    "    for w in s.split():\n",
    "        if w not in words_text_te:\n",
    "            words_text_te[w] = 1\n",
    "        words_text_te[w] = words_text_te[w] + 1\n",
    "print(f'Different words in training: {len(words_text_te)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = list(words_text_tr.keys())\n",
    "te = list(words_text_te.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in te:\n",
    "    if(w in commonverbs or w in frequent):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in te:\n",
    "    if(w in tr):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(te)-set(tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(set(tr)-set(te)):\n",
    "    if(i.find('rimbor')>0):\n",
    "        print(i, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Different words in training: {len(words_text_tr)}')\n",
    "print({k: v for k, v in sorted(words_text_tr.items(), key=lambda item: item[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Different words in training: {len(words_text)}')\n",
    "print({k: v for k, v in sorted(words_text.items(), key=lambda item: item[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate TF-IDF\n",
    "\n",
    "#'clean', 'Job_offer'\n",
    "column = 'clean'\n",
    "max_feature = len(train_ds[column])\n",
    "# print(f'#Feature {max_feature}')\n",
    "\n",
    "def X_tfidf(sentences,max_feature = max_feature):  \n",
    "    tfidf = TfidfVectorizer(min_df=2, max_df=0.2, ngram_range=(1,3),lowercase=True)#,strip_accents='ascii'\n",
    "    X = tfidf.fit_transform(sentences)\n",
    "    return X, tfidf\n",
    "\n",
    "train_vec, vectorizer = X_tfidf(train_ds[column])\n",
    "test_vec = vectorizer.transform(test_ds[column])\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_vec, train_ds['Label'], test_vec, test_ds['Label']\n",
    "print(f'X_train, y_train, X_test, y_test', len(X_train.todense()), len(y_train), len(X_test.todense()), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vectorizer.get_feature_names()))\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(classifier,X_test,y_test):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_score_ = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"#Precision: {precision:.5f}\")\n",
    "    print(f\"#Recall: {recall:.5f}\")\n",
    "    print(f\"#f1 Score: {f1_score_:.5f}\")\n",
    "    return precision,recall,f1_score_,y_pred\n",
    "\n",
    "def train_model(classifier, X_train, X_test, y_train, y_test, printAll=False):\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    precision,recall,f1_score_,y_pred = get_score(classifier,X_test,y_test)\n",
    "        \n",
    "    if(printAll):\n",
    "        print(confusion_matrix(y_test,y_pred))      \n",
    "        print(classification_report(y_test,y_pred))\n",
    "    print(\"#--- --- ---\")\n",
    "    return precision,recall,f1_score_,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "# max = [{0:0},{0:0},{0:0}]\n",
    "# for c in range(1,20,1):\n",
    "#     print(c)\n",
    "#     lsvc = LinearSVC(tol=1e-8,max_iter=10000,random_state=0,C=c/10, penalty='l2')\n",
    "#     p,r,f,_ = train_model(lsvc, X_train, X_test, y_train, y_test)\n",
    "#     if(p>list(max[0].values())[0]):\n",
    "#         max[0] = {c:p}\n",
    "#     if(r>list(max[1].values())[0]):\n",
    "#         max[1] = {c:r}\n",
    "#     if(f>list(max[2].values())[0]):\n",
    "#         max[2] = {c:f}\n",
    "\n",
    "c=.5 #best\n",
    "lsvc = LinearSVC(tol=1e-9,max_iter=10**8,random_state=0,C=5/10, penalty='l2')\n",
    "_ = train_model(lsvc, X_train, X_test, y_train, y_test)\n",
    "#Precision: 0.83988\n",
    "#Recall: 0.84042\n",
    "#f1 Score: 0.83933\n",
    "#--- --- ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max = [{0:0},{0:0},{0:0}]\n",
    "# for c in range(1,50):\n",
    "#     print(c)\n",
    "#     lr = LogisticRegression(penalty='l2',max_iter=10**6,C=c/10,random_state=0, solver='liblinear')\n",
    "#     p,r,f,_ = train_model(lr, X_train, X_test, y_train, y_test)\n",
    "#     if(p>list(max[0].values())[0]):\n",
    "#         max[0] = {c:p}\n",
    "#     if(r>list(max[1].values())[0]):\n",
    "#         max[1] = {c:r}\n",
    "#     if(f>list(max[2].values())[0]):\n",
    "#         max[2] = {c:f}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "c=4.1 #best\n",
    "#{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}\n",
    "lr = LogisticRegression(penalty='l2',max_iter=10**9,C=c,random_state=10, solver='liblinear')\n",
    "_,_,_,y_pred = train_model(lr, X_train, X_test, y_train, y_test)\n",
    "#Precision: 0.85283\n",
    "#Recall: 0.84650\n",
    "#f1 Score: 0.84918\n",
    "#--- --- ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# max = [{0:0},{0:0},{0:0}]\n",
    "# for c in range(1,100):\n",
    "#     print(c)\n",
    "#     lr = LogisticRegression(penalty='l2',max_iter=10**6,C=c/10,random_state=0, solver='liblinear')\n",
    "#     p,r,f = train_model(lr, X_train, X_test, y_train, y_test)\n",
    "#     if(p>list(max[0].values())[0]):\n",
    "#         max[0] = {c:p}\n",
    "#     if(r>list(max[1].values())[0]):\n",
    "#         max[1] = {c:r}\n",
    "#     if(f>list(max[2].values())[0]):\n",
    "#         max[2] = {c:f}\n",
    "        \n",
    "c=2 #best\n",
    "#{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}\n",
    "lr = LogisticRegression(penalty='l2',max_iter=10**7,C=c,random_state=10, solver='liblinear')\n",
    "_,_,_,y_pred = train_model(lr, X_train, X_test, y_train, y_test)\n",
    "#Precision: 0.85283\n",
    "#Recall: 0.84650\n",
    "#f1 Score: 0.84918\n",
    "#--- --- ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "for i in range(10,110,10):\n",
    "    print(i/100)\n",
    "    mNB = MultinomialNB(alpha=i/100)\n",
    "    print(f'Classifier MultinomialNB')\n",
    "    train_model(mNB, X_train.todense(), X_test.todense(), y_train, y_test)\n",
    "    bNB = BernoulliNB(alpha=i/100)\n",
    "    print(f'Classifier BernoulliNB')\n",
    "    train_model(bNB, X_train.todense(), X_test.todense(), y_train, y_test)\n",
    "    print(\"--- ---- ----\")\n",
    "gNB = GaussianNB()\n",
    "print(f'Classifier GaussianNB')\n",
    "train_model(gNB, X_train.todense(), X_test.todense(), y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#'gini', 'entropy'\n",
    "classifier = RandomForestClassifier(n_estimators=512*2, random_state=0,max_depth=None,criterion='gini')\n",
    "train_model(classifier, X_train, X_test, y_train, y_test,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#perceptron, hinge, log, squared_hinge\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', validation_fraction=0.1,max_iter=3*10**6,tol=10**-8,random_state=0)\n",
    "train_model(sgd, X_train, X_test, y_train, y_test)\n",
    "#Precision: 0.81388\n",
    "#Recall: 0.81592\n",
    "#f1 Score: 0.81442\n",
    "#--- --- ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=256,tol = 0.001, verbose = True, n_iter_no_change = 3, max_iter=500)\n",
    "train_model(model, X_train, X_test, y_train, y_test)\n",
    "#Precision: 0.83057\n",
    "#Recall: 0.82624\n",
    "#f1 Score: 0.82788\n",
    "#--- --- ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "n_estimators = 512\n",
    "# clf1 = RandomForestClassifier(n_estimators=n_estimators, random_state=0,max_depth=None,criterion='gini')\n",
    "# clf2 = LogisticRegression(penalty='l2',max_iter=10**6,C=4.1,random_state=1)\n",
    "# clf3 = LinearSVC(tol=1e-8,max_iter=10000,random_state=0,C=5/10, penalty='l2')\n",
    "# clf4 = LinearSVC(tol=1e-8,max_iter=10000,random_state=1,C=.5, penalty='l2')\n",
    "clf5 = LogisticRegression(penalty='l2',max_iter=10**6,C=4.1,random_state=10)\n",
    "clf51 = LogisticRegression(penalty='l2',max_iter=10**6,C=2.5,random_state=10)\n",
    "clf52 = LogisticRegression(penalty='l2',max_iter=10**6,C=4,random_state=10)\n",
    "# clf6 = SGDClassifier(loss='hinge', penalty='l2', validation_fraction=0.1,max_iter=3*10**6,tol=10**-8,random_state=0)\n",
    "#('clf1', clf1), ('clf2', clf2), ('clf3', clf3), ('clf4', clf4),('clf6', clf6), ('clf52', clf52)\n",
    "VCclf = VotingClassifier(estimators=[('clf5', clf5), ('clf51', clf51)], voting='soft')\n",
    "train_model(VCclf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('prediction.csv','w')\n",
    "# file.write('Job_description;Label_true;Label_pred\\n')\n",
    "for i in zip(test_ds['Job_offer'],y_test,y_pred):\n",
    "    file.write(f'{i[0]};{i[1]};{i[2]}\\n')\n",
    "#     if(i[1]!=i[2]):\n",
    "        print(f'{i[0]};{i[1]};{i[2]}\\n')\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Job_description', 'Label_true', 'Label_pred']\n",
    "df = pd.DataFrame(list(zip(test_ds['Job_offer'],y_test,y_pred)),columns=columns)\n",
    "df.to_csv('prediction.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ds = pd.read_csv('prediction.csv', sep=';', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "_ = get_score(loaded_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def Word_Cloud(words):\n",
    "    text = \"\"\n",
    "    for i in words[1:-1]:\n",
    "        if len(i[0].split(' ')) ==1:\n",
    "             text = text + \" \" + i[0]\n",
    "    plt.figure( figsize=(20,10), facecolor='k', frameon=False)\n",
    "    wordcloud= WordCloud(width=1200, height=600,min_font_size=8, max_font_size=100, max_words=500, background_color=\"white\", contour_width=0,contour_color='white').generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_Cloud(list(vectorizer.vocabulary_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 5000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(train_ds['clean'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train_ds['clean'])\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X_train.shape)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test_ds['clean'])\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(train_ds['Label']).values\n",
    "print('Shape of label tensor:', y_train.shape)\n",
    "\n",
    "y_test = pd.get_dummies(test_ds['Label']).values\n",
    "print('Shape of label tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.25, recurrent_dropout=0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = tf.argmax(y_test,1)\n",
    "y_pred = tf.argmax(y_pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1_score_ = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"#Precision: {precision:.5f}\")\n",
    "print(f\"#Recall: {recall:.5f}\")\n",
    "print(f\"#f1 Score: {f1_score_:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
